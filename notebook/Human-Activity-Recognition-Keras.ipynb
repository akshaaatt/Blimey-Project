{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-4-2dbe34bebd9c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mseaborn\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0msns\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use \"Sensors Activity Dataset\" by Shoaib et al. which is available for download from [here](https://www.utwente.nl/en/eemcs/ps/research/dataset/).\n",
    "There are 7 activities in this dataset: Biking, Downstairs, Jogging, Sitting, Standing, Upstairs, Walking. \n",
    "There were ten participants involved in data collection experiment who performed  each of these activities for 3-4 minutes.\n",
    "All ten participants were male, between the ages of 25 and 30. \n",
    "Each of these participants was equipped with five smartphones on five body positions: \n",
    "\n",
    "1. One in their right jean’s pocket. \n",
    "2. One in their left jean’s pocket.\n",
    "3. One on belt position towards the right leg using a belt clipper.\n",
    "4. One on the right upper arm. \n",
    "5. One on the right wrist.\n",
    "\n",
    "The data was collected for an accelerometer, a gyroscope, a magnetometer, and a linear acceleration sensor. \n",
    "Each csv file contains data for each participant's seven physical activities for all five positions. \n",
    "\n",
    "Notation in these files: \n",
    "\n",
    "Accelerometer ( Ax = x-axis, Ay = y-axis, Az= Z-aixs)   \n",
    "Linear Acceleration Sensor ( Lx = x-axis, Ly = y axis, Lz= Z-aixs)   \n",
    "Gyroscope ( Gx = x-axis, Gy = y-axis, Gz= Z-aixs)   \n",
    "Magnetometer ( Mx = x-axis, My = y-axis, Mz= Z-aixs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "! wget https://www.utwente.nl/en/eemcs/ps/dataset-folder/sensors-activity-recognition-dataset-shoaib.rar -P ../data/\n",
    "\n",
    "# Extract dataset using unrar\n",
    "!pip install unrar\n",
    "!unrar e ../data/sensors-activity-recognition-dataset-shoaib.rar ../data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load all 10 participants data into a single dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    df_tmp = pd.read_csv('../data/Participant_' + str(i+1) + '.csv', header=1)\n",
    "    df = pd.concat([df, df_tmp])\n",
    "\n",
    "# View top 5 rows of dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split data into train and test sets (80% train, 20% test):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(len(df) * 0.8)\n",
    "train_data = df.iloc[:split_point, :]\n",
    "test_data = df.iloc[split_point:, :]\n",
    "\n",
    "print(\"Number of train spamples: \", len(train_data))\n",
    "print(\"Number of test spamples: \", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we only use right pocket's and left pocket's data, we should concatenate those into a single data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(data):\n",
    "    \n",
    "    # Select left pocket data\n",
    "    left_pocket = data.iloc[:,1:10]\n",
    "    \n",
    "    #Square root of sum of squares of accelerometer, linear acceleration and gyroscope data\n",
    "    left_pocket[\"MA\"] = np.sqrt(np.square(left_pocket['Ax']) + np.square(left_pocket['Ay']) + np.square(left_pocket['Az']))\n",
    "    left_pocket[\"ML\"] = np.sqrt(np.square(left_pocket['Lx']) + np.square(left_pocket['Ly']) + np.square(left_pocket['Lz']))\n",
    "    left_pocket[\"MG\"] = np.sqrt(np.square(left_pocket['Gx']) + np.square(left_pocket['Gy']) + np.square(left_pocket['Gz']))\n",
    "    \n",
    "\n",
    "    # Select right pocket data\n",
    "    right_pocket = data.iloc[:,15:24]\n",
    "    right_pocket.columns=['Ax', 'Ay', 'Az', 'Lx', 'Ly', 'Lz', 'Gx', 'Gy', 'Gz']\n",
    "    \n",
    "    #Square root of sum of squares of accelerometer, linear acceleration and gyroscope data\n",
    "    right_pocket[\"MA\"] = np.sqrt(np.square(right_pocket['Ax']) + np.square(right_pocket['Ay']) + np.square(right_pocket['Az']))\n",
    "    right_pocket[\"ML\"] = np.sqrt(np.square(right_pocket['Lx']) + np.square(right_pocket['Ly']) + np.square(right_pocket['Lz']))\n",
    "    right_pocket[\"MG\"] = np.sqrt(np.square(right_pocket['Gx']) + np.square(right_pocket['Gy']) + np.square(right_pocket['Gz']))\n",
    "\n",
    "    \n",
    "    # Extract labels \n",
    "    labels = data.iloc[:, 69] \n",
    "    labels = labels.to_frame()\n",
    "    labels.columns=['Activity_Label']\n",
    "    labels = pd.concat([labels]*2, ignore_index=True)\n",
    "    #replace typo 'upsatirs' with upstairs! \n",
    "    labels.loc[(labels['Activity_Label'] == 'upsatirs')] = 'upstairs'\n",
    "    \n",
    "    #Concatenate left pocket and right pocket data into a single data frame (we only use left pocket and right pocket data)\n",
    "    frames = [left_pocket, right_pocket]\n",
    "    df = pd.concat(frames)\n",
    "   \n",
    "    return df, labels\n",
    "\n",
    "# Generate input data and labels\n",
    "train_X, train_y = concat(train_data)\n",
    "test_X, test_y = concat(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use sliding window mechanism to generate data segments.\n",
    "We use Accelerometer, Linear acceleration and Gyroscope features and their sum of squares roots:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TIME_STEPS = 100 #sliding window length\n",
    "STEP = 50 #Sliding window step size\n",
    "N_FEATURES = 12 \n",
    "\n",
    "def generate_sequence(x, y, n_time_steps, step):\n",
    "    \n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(0, len(x) - n_time_steps, step):\n",
    "        ax = x['Ax'].values[i: i + n_time_steps]\n",
    "        ay = x['Ay'].values[i: i + n_time_steps]\n",
    "        az = x['Az'].values[i: i + n_time_steps]\n",
    "\n",
    "        lx = x['Lx'].values[i: i + n_time_steps]\n",
    "        ly = x['Ly'].values[i: i + n_time_steps]\n",
    "        lz = x['Lz'].values[i: i + n_time_steps]\n",
    "        \n",
    "        gx = x['Gx'].values[i: i + n_time_steps]\n",
    "        gy = x['Gy'].values[i: i + n_time_steps]\n",
    "        gz = x['Gz'].values[i: i + n_time_steps]\n",
    "\n",
    "        MA = x['MA'].values[i: i + n_time_steps]\n",
    "        ML = x['ML'].values[i: i + n_time_steps]\n",
    "        MG = x['MG'].values[i: i + n_time_steps]\n",
    "       \n",
    "        label = stats.mode(y['Activity_Label'][i: i + n_time_steps])[0][0]\n",
    "        segments.append([ax, ay, az, lx, ly, lz, gx, gy, gz, MA, ML, MG])\n",
    "        labels.append(label)\n",
    "        \n",
    "    return segments, labels\n",
    "\n",
    "train_X, train_y = generate_sequence(train_X, train_y, N_TIME_STEPS, STEP)\n",
    "test_X, test_y = generate_sequence(test_X, test_y, N_TIME_STEPS, STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input segments and one-hot encode labels\n",
    "def reshape_segments(x, y, n_time_steps, n_features):\n",
    "    \n",
    "    x_reshaped = np.asarray(x, dtype= np.float32).reshape(-1, n_time_steps, n_features)\n",
    "    y_reshaped = np.asarray(pd.get_dummies(y), dtype = np.float32)\n",
    "    return x_reshaped, y_reshaped\n",
    "\n",
    "X_train, y_train = reshape_segments(train_X, train_y, N_TIME_STEPS, N_FEATURES)\n",
    "X_test, y_test = reshape_segments(test_X, test_y, N_TIME_STEPS, N_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras import optimizers\n",
    "\n",
    "N_CLASSES = 7\n",
    "N_HIDDEN_UNITS = 32\n",
    "L2 = 0.000001\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(N_HIDDEN_UNITS, return_sequences=True, input_shape=(N_TIME_STEPS, N_FEATURES), \n",
    "         kernel_initializer='orthogonal', kernel_regularizer=l2(L2), recurrent_regularizer=l2(L2),\n",
    "         bias_regularizer=l2(L2), name=\"LSTM_1\"),\n",
    "    Flatten(name='Flatten'),\n",
    "    Dense(N_HIDDEN_UNITS, activation='relu', kernel_regularizer=l2(L2), bias_regularizer=l2(L2), name=\"Dense_1\"),\n",
    "    Dense(N_CLASSES, activation='softmax', kernel_regularizer=l2(L2), bias_regularizer=l2(L2), name=\"Dense_2\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.RMSprop(lr=0.0001)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "N_EPOCHS = 30\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=BATCH_SIZE, epochs=N_EPOCHS,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ohe = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred_ohe, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "confusion_matrix = metrics.confusion_matrix(y_true=y_true_labels, y_pred=y_pred_labels)\n",
    "\n",
    "LABELS = ['Biking' ,' Downstairs', 'Jogging', 'Sitting', 'Standing', 'Upstairs', 'Walking']\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "sns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we export the trained model in a format which easily can be used in our android application for on-device inference. The exported model will be named \"frozen_HAR.pb\" and be stored under models directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "\n",
    "input_node_names= [\"LSTM_1_input\"]\n",
    "output_node_name = \"Dense_2/Softmax\"\n",
    "MODEL_NAME = \"HAR\"\n",
    "\n",
    "tf.train.write_graph(K.get_session().graph_def, 'models', \\\n",
    "        MODEL_NAME + '_graph.pbtxt')\n",
    "saver = tf.train.Saver()\n",
    "saver.save(K.get_session(), 'models/' + MODEL_NAME + '.chkp')\n",
    "\n",
    "freeze_graph.freeze_graph('models/' + MODEL_NAME + '_graph.pbtxt', None, \\\n",
    "    False, 'models/' + MODEL_NAME + '.chkp', output_node_name, \\\n",
    "    \"save/restore_all\", \"save/Const:0\", \\\n",
    "    'models/frozen_' + MODEL_NAME + '.pb', True, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}